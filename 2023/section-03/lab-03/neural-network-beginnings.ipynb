{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -U lightning -q"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Neural Network Beginnings\n",
    "\n",
    "In the world of machine learning and artificial intelligence, neural networks are a class of models inspired by the human brain. They are composed of layers of nodes, or \"neurons\", with each layer transforming its input data in a way that allows the network to learn from experience. Initially, neural networks were introduced to allow computers to recognize patterns, but they have since become a fundamental tool for a wide range of machine learning tasks.\n",
    "\n",
    "![Neuron Diagram](single-neuron.jpg)\n",
    "\n",
    "## A Single Neuron\n",
    "\n",
    "A single neuron, also known as a perceptron, is the basic unit of a neural network. It takes a set of numerical inputs, combines them in a weighted sum, and passes them through an activation function to produce an output. Here's a breakdown of these components:\n",
    "\n",
    "- **Inputs:** These are the data that are fed into the neuron. For example, in a neural network that processes images, the inputs could be the pixel values of an image.\n",
    "\n",
    "- **Weights:** Each input is associated with a weight that represents its importance. The neuron learns the correct weights during the training process. Initially, these weights are often set randomly.\n",
    "\n",
    "- **Biases:** The bias allows the neuron to have some flexibility in activation. It is a constant term that is added to the weighted sum of the inputs before the activation function is applied. Like the weights, biases are learned during training.\n",
    "\n",
    "- **Activation Function:** After the weighted sum of the inputs and the bias are calculated, this sum is passed through an activation function. The purpose of this function is to introduce non-linearity into the system, allowing the neural network to learn from the error of its predictions and make necessary corrections. Common activation functions include ReLU (Rectified Linear Unit), Sigmoid, and Tanh.\n",
    "\n",
    "In mathematical terms, the output $y$ of a single neuron can be described as follows:\n",
    "$y = f(w_1 x_1 + w_2 x_2 + \\cdots + w_n x_n + b)$\n",
    "where:\n",
    "$w_1, w_2, \\ldots, w_n$ are the weights,\n",
    "$x_1, x_2, \\ldots, x_n$ are the inputs,\n",
    "$b$ is the bias, and\n",
    "$f$ is the activation function.\n",
    "\n",
    "This simple structure, when scaled to hundreds or thousands of neurons arranged in layers, enables neural networks to approximate complex and highly non-linear functions, making them a powerful tool in machine learning.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pytorch_lightning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Perceptron(pytorch_lightning.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define the single linear layer (4 input features, 1 output)\n",
    "        self.layer = nn.Linear(4, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the linear layer\n",
    "        # then through the sigmoid activation function\n",
    "        return torch.sigmoid(self.layer(x))\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.binary_cross_entropy(y_hat, y.view(-1, 1).float())\n",
    "        self.log(\"train_loss\", loss)  # Logging the loss\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # We will use the Adam optimizer\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Generate synthetic data for the example\n",
    "numpy.random.seed(42)\n",
    "N = 1000\n",
    "temp = numpy.random.uniform(0, 100, N)\n",
    "rain = numpy.random.randint(0, 2, N)\n",
    "ride = numpy.random.randint(0, 2, N)\n",
    "friends = numpy.random.randint(0, 2, N)\n",
    "X = numpy.vstack([temp, rain, ride, friends]).T\n",
    "# Assume that the individual will go to the zoo if there is a ride and friends are going\n",
    "Y = (ride & friends).astype(numpy.float32)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create TensorDatasets and DataLoaders\n",
    "train_data = TensorDataset(\n",
    "    torch.tensor(X_train, dtype=torch.float32),\n",
    "    torch.tensor(Y_train, dtype=torch.float32),\n",
    ")\n",
    "val_data = TensorDataset(\n",
    "    torch.tensor(X_val, dtype=torch.float32), torch.tensor(Y_val, dtype=torch.float32)\n",
    ")\n",
    "train_loader = DataLoader(train_data, batch_size=32)\n",
    "val_loader = DataLoader(val_data, batch_size=32)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Start TensorBoard.\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model = Perceptron()\n",
    "trainer = pytorch_lightning.Trainer(max_epochs=50, log_every_n_steps=3)\n",
    "trainer.fit(model, train_loader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Generate some new data to test\n",
    "test_temp = numpy.array([75, 85, 60, 90, 72])\n",
    "test_rain = numpy.array([0, 1, 0, 1, 0])\n",
    "test_ride = numpy.array([1, 0, 1, 0, 1])\n",
    "test_friends = numpy.array([1, 1, 0, 0, 1])\n",
    "X_test = numpy.vstack([test_temp, test_rain, test_ride, test_friends]).T\n",
    "\n",
    "# Convert to a PyTorch tensor\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test_tensor)\n",
    "\n",
    "# Print predictions\n",
    "for i, pred in enumerate(predictions):\n",
    "    print(\n",
    "        f\"Conditions: {X_test[i]} | Probability of going to the zoo: {pred.item():.4f}\"\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
