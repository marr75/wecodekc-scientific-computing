{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -q condacolab\n",
    "import condacolab\n",
    "condacolab.install() # expect a kernel restart"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6557a368d9638dd0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!mamba install sentence-transformers faiss wikipedia pandas -yq"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7501dc7f85e2617"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Your Own Embedding Model\n",
    "\n",
    "Up to this point, you've been using pre-trained models, which are a bit like using someone else's map to navigate a city. Now, we're going to draw our own map by training an embedding model on a dataset of our choosing. This is an exciting opportunity to tailor the model to better understand the specific topics and language that you're interested in.\n",
    "\n",
    "### Customizing Your Model\n",
    "\n",
    "When training your own model, there are several levers you can pull to potentially enhance its performance:\n",
    "\n",
    "1. **Increase Training Data**: The more examples your model sees, the better it can learn. You can use the `wikipedia` package to fetch articles on a range of topics. More diverse and extensive data can lead to a more robust model.\n",
    "\n",
    "2. **Model Architecture**: You can change the underlying architecture of the model by specifying a different model string when initializing `SentenceTransformer`. Experiment with different architectures like `bert-base-nli-mean-tokens`, `roberta-base-nli-stsb-mean-tokens`, or even larger models if you have the computational resources.\n",
    "\n",
    "3. **Training Duration**: The amount of time you train your model (number of epochs) also impacts performance. More training can result in a better understanding of the text, but also watch out for overfittingâ€”where the model learns the training data too well and doesn't generalize to new data.\n",
    "\n",
    "4. **Loss Function**: The loss function you choose tells the model how to measure its mistakes during training. Different tasks might benefit from different loss functions, so feel free to experiment with options like `ContrastiveLoss`, `MultipleNegativesRankingLoss`, or `TripletLoss`.\n",
    "\n",
    "5. **Evaluation**: Remember to evaluate your model regularly during training. This helps you understand whether the changes you're making are improving performance.\n",
    "\n",
    "### Your Challenge\n",
    "\n",
    "Train your own model using the provided code snippet as a starting point. Fetch more articles from Wikipedia on topics you're interested in, configure the model and training parameters, and let the training begin! Keep an eye on how changes in these configurations affect your model's understanding of language.\n",
    "\n",
    "Happy modeling, and may the best embeddings win!\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c06a9c3847d6778d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# imports from the first lab\n",
    "import wikipedia\n",
    "import sentence_transformers, sentence_transformers.losses\n",
    "import faiss\n",
    "import numpy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "689d3456149bb6cb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Prepare the dataset\n",
    "train_examples = [sentence_transformers.InputExample(texts=[\n",
    "    'First sentence.',\n",
    "    'Second sentence.',\n",
    "], label=0.8)]\n",
    "\n",
    "# Define the model\n",
    "model = sentence_transformers.SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "\n",
    "# Define a dataloader and loss\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "train_loss = sentence_transformers.losses.CosineSimilarityLoss(model)\n",
    "\n",
    "# Training\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, warmup_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Show the model architecture\n",
    "model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0902567c7bb45b8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Show the example output\n",
    "model.encode(['First sentence.', 'Second sentence.'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "740372abbb57c168"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Functions from the first notebook\n",
    "def get_articles_by_topic(topics):\n",
    "    # Step 1: Fetch articles\n",
    "    articles = {topic: wikipedia.page(topic).content for topic in topics}\n",
    "\n",
    "    # Step 2: Preprocess text\n",
    "    # (assuming simple preprocessing for demonstration)\n",
    "    processed_articles = {\n",
    "        title: content.replace(\"\\n\", \" \") for title, content in articles.items()\n",
    "    }\n",
    "    return processed_articles\n",
    "\n",
    "# Prepare a function to create a new index\n",
    "def create_index(passages, model, instruction=\"passage\"):\n",
    "    if instruction:\n",
    "        passages = [\n",
    "            f\"{instruction}: {passage}\" for passage in passages\n",
    "        ]\n",
    "    # Step 3: Generate embeddings\n",
    "    embeddings = [\n",
    "        model.encode(content, normalize_embeddings=True)\n",
    "        for content in passages\n",
    "    ]\n",
    "\n",
    "    # Step 4: Indexing with FAISS\n",
    "    # Get the size of the embeddings\n",
    "    dimension = (\n",
    "        embeddings[0].shape[0]\n",
    "    )\n",
    "    # Use the \"distance\" for the index\n",
    "    index = faiss.IndexFlatIP(dimension)\n",
    "\n",
    "    # You need to convert the embeddings dictionary to a list of embeddings\n",
    "    embeddings_matrix = numpy.array(embeddings)\n",
    "    index.add(embeddings_matrix)  # Add embeddings to the index\n",
    "\n",
    "    # return the results\n",
    "    return index\n",
    "\n",
    "def search(query, model, index, k=3, instruction=\"query\"):\n",
    "    \"\"\"\n",
    "    Search for relevant articles given a query.\n",
    "    Some models need a special instruction (e.g. \"query: \")\n",
    "    \"\"\"\n",
    "    # Need to embed the query\n",
    "    if instruction:\n",
    "        query = f\"{instruction}: {query}\"\n",
    "    query_embedding = model.encode(query)\n",
    "    # k=3 finds the 3 closest article\n",
    "    distances, indices = index.search(numpy.array([query_embedding]), k=k)\n",
    "    return distances, indices"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e6ab70a147d2b3f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save the model if you like it\n",
    "# model.save('path-to-save-model/')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c6753cceec08013"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
