{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21d0dfb4",
   "metadata": {},
   "source": [
    "# Introduction to Word Embeddings Lab\n",
    "\n",
    "Welcome to the Word Embeddings Lab, where you will delve into the fascinating world of machine learning and natural language processing (NLP). In this session, we aim to demystify how machines understand and process human language. You'll learn about embeddings, which are a cornerstone in the field of NLP, and you'll see how they can be used to create a semantic search engine.\n",
    "\n",
    "## What Will You Learn?\n",
    "\n",
    "- **Word Embeddings**: Understand what word embeddings are and why they are a powerful tool for representing text in a way that captures the meaning and relationships between words.\n",
    "- **Semantic Search**: Build a semantic search engine that can find relevant articles based on the meaning of a search query, rather than just keyword matching.\n",
    "- **FAISS**: Get introduced to FAISS (Facebook AI Similarity Search), a library for efficient similarity searching.\n",
    "\n",
    "## Why Are These Concepts Important?\n",
    "\n",
    "- **Machine Understanding**: Word embeddings allow computers to process text in a more human-like way, understanding context and semantics.\n",
    "- **Applicability**: The concepts you learn here are used in a variety of applications, from recommendation systems to automated customer support.\n",
    "- **Real-world Tools**: You will work with real-world tools that professionals use for machine learning projects, including pre-trained models from Hugging Face's Transformers library.\n",
    "\n",
    "## Before You Start:\n",
    "\n",
    "Remember that the field of AI is about experimentation and innovation. Don't be afraid to try new things and ask questions. The goal is to learn and explore, even if things don't work perfectly the first time. Now, let's embark on this journey through the world of AI and NLP together!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d9b244",
   "metadata": {},
   "source": [
    "### Setup: environment\n",
    "\n",
    "This cell sets up a conda environment in Google Colab, which allows us to install and manage packages we'll need for our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "initial_id",
    "outputId": "4dcaeac5-7959-4041-c103-a4ab14b56d8a"
   },
   "outputs": [],
   "source": [
    "!pip install -q condacolab\n",
    "import condacolab\n",
    "condacolab.install() # expect a kernel restart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd834acd",
   "metadata": {},
   "source": [
    "### Setup: package installs\n",
    "\n",
    "Here we install several important packages:\n",
    "- `sentence-transformers`: For working with state-of-the-art sentence embeddings.\n",
    "- `faiss`: For efficient similarity searches.\n",
    "- `wikipedia`: To easily access and download Wikipedia articles.\n",
    "- `pandas`: For organizing and manipulating data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "A1dQEFFfUG4e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A1dQEFFfUG4e",
    "outputId": "b6fbc6b1-64f8-4e83-91f1-fcc0d0eb67ea"
   },
   "outputs": [],
   "source": [
    "!mamba install sentence-transformers faiss wikipedia pandas -yq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b2213f",
   "metadata": {},
   "source": [
    "### Setup: package imports\n",
    "\n",
    "After installing the necessary packages, we import them into our notebook. This gives us the tools we need to start working on our machine learning project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50W4RQ4gUmQt",
   "metadata": {
    "id": "50W4RQ4gUmQt"
   },
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import sentence_transformers\n",
    "import faiss\n",
    "import numpy\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparing the Embedding Model\n",
    "\n",
    "This section disables logging for Transformers to keep the output clean. We then initialize a pre-trained model for generating embeddings."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8cf53e426b4faf9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WK_22cNbV1EU",
   "metadata": {
    "id": "WK_22cNbV1EU"
   },
   "outputs": [],
   "source": [
    "# Set Transformers' logging to error only to suppress download messages\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "# Prepare an embedding model\n",
    "model = sentence_transformers.SentenceTransformer(\"intfloat/e5-small-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fetching and Indexing Articles\n",
    "\n",
    "Here we define two functions:\n",
    "- `get_articles_by_topic`: To fetch and preprocess Wikipedia articles.\n",
    "- `create_index`: To create a FAISS index with the articles' embeddings."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d836c8e61876de9b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_articles_by_topic(topics):\n",
    "    # Step 1: Fetch articles\n",
    "    articles = {topic: wikipedia.page(topic).content for topic in topics}\n",
    "\n",
    "    # Step 2: Preprocess text\n",
    "    # (assuming simple preprocessing for demonstration)\n",
    "    processed_articles = {\n",
    "        title: content.replace(\"\\n\", \" \") for title, content in articles.items()\n",
    "    }\n",
    "    return processed_articles\n",
    "\n",
    "# Prepare a function to create a new index\n",
    "def create_index(passages, model, instruction=\"passage\"):\n",
    "    if instruction:\n",
    "        passages = [\n",
    "            f\"{instruction}: {passage}\" for passage in passages\n",
    "        ]\n",
    "    # Step 3: Generate embeddings\n",
    "    embeddings = [\n",
    "        model.encode(content, normalize_embeddings=True)\n",
    "        for content in passages\n",
    "    ]\n",
    "\n",
    "    # Step 4: Indexing with FAISS\n",
    "    # Get the size of the embeddings\n",
    "    dimension = (\n",
    "        embeddings[0].shape[0]\n",
    "    )\n",
    "    # Use the \"distance\" for the index\n",
    "    index = faiss.IndexFlatIP(dimension)\n",
    "\n",
    "    # You need to convert the embeddings dictionary to a list of embeddings\n",
    "    embeddings_matrix = numpy.array(embeddings)\n",
    "    index.add(embeddings_matrix)  # Add embeddings to the index\n",
    "\n",
    "    # return the results\n",
    "    return index"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5dc9c97e7e9ecd39"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fetching and Indexing Articles\n",
    "\n",
    "We'll fetch some articles by topic and create an index for them."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40b798aec06c7480"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qiiLeiJuaPjn",
   "metadata": {
    "id": "qiiLeiJuaPjn"
   },
   "outputs": [],
   "source": [
    "topics = [\n",
    "    \"Earth\",\n",
    "    \"Computer Science\",\n",
    "    \"Artificial Intelligence\",\n",
    "    \"Python (programming language)\",\n",
    "    \"Leonardo da Vinci\",\n",
    "    \"Eiffel Tower\",\n",
    "]\n",
    "articles = get_articles_by_topic(topics)\n",
    "index = create_index(articles, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Semantic Search Function\n",
    "\n",
    "In this part, we implement the semantic search function. It allows us to search our indexed articles with a natural language query."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "243d58b35aed4406"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yLdl0Zt3a_DV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yLdl0Zt3a_DV",
    "outputId": "296f7d84-7c6e-49da-a64a-e1bc33f550ef"
   },
   "outputs": [],
   "source": [
    "# Step 5: Semantic search\n",
    "def search(query, model, k=3, instruction=\"query\"):\n",
    "    \"\"\"\n",
    "    Search for relevant articles given a query.\n",
    "    Some models need a special instruction (e.g. \"query: \")\n",
    "    \"\"\"\n",
    "    # Need to embed the query\n",
    "    if instruction:\n",
    "        query = f\"{instruction}: {query}\"\n",
    "    query_embedding = model.encode(query)\n",
    "    # k=3 finds the 3 closest article\n",
    "    distances, indices = index.search(numpy.array([query_embedding]), k=k)\n",
    "    return distances, indices\n",
    "\n",
    "# Step 6: Present results\n",
    "query = \"I want to learn about historical landmarks in Europe\"\n",
    "distances, indices = search(query, model)\n",
    "\n",
    "for i, idx in enumerate(indices[0]):\n",
    "    print(f\"Article title: {list(articles.keys())[idx]}\")\n",
    "    print(f\"Distance: {distances[0][i]}\")\n",
    "    print(f\"Snippet: {articles[list(articles.keys())[idx]][:100]}...\")  # Display the first 100 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exploring Semantic Search with New Topics\n",
    "\n",
    "Now that you have learned how to create a semantic search engine using word embeddings and FAISS, it's time to put your new skills into practice!\n",
    "\n",
    "### Your Challenge:\n",
    "\n",
    "1. **Select New Topics**: Choose 10-15 new topics that interest you. These can be anything from your favorite sport, a historical figure you admire, to a science concept you're curious about.\n",
    "\n",
    "2. **Fetch and Index**: Use the `get_articles_by_topic` function to fetch the Wikipedia articles for your chosen topics and then create a new index using the `create_index` function.\n",
    "\n",
    "3. **Craft Your Search Queries**: Think about what you want to learn from these articles. Formulate 3-5 search queries that reflect your interests or questions.\n",
    "\n",
    "4. **Search and Discover**: Use the `search` function with your queries to see which articles are most relevant to your questions. Examine the results and see if the articles answer your questions or if they lead to new questions.\n",
    "\n",
    "5. **Reflect and Share**: After you perform your searches, take some time to reflect on the results.\n",
    "   - Were the articles what you expected?\n",
    "   - Did you find the information you were looking for?\n",
    "   - Share your findings and insights with the class.\n",
    "\n",
    "This is your opportunity to explore the vast knowledge contained in Wikipedia using the power of AI. Have fun searching!\n",
    "\n",
    "### Tips for Success:\n",
    "\n",
    "- Be specific with your search queries to get the best results.\n",
    "- If your first search doesn't return what you expected, try rephrasing your query or choosing different keywords.\n",
    "- Remember that the way you phrase your query can greatly influence the search results.\n",
    "\n",
    "Happy Exploring!\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10d3280d3e81657a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3505fbdd53888f1"
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
