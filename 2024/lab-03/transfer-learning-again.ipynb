{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df2610a1",
   "metadata": {},
   "source": [
    "# Lab: Transfer Learning and Fine-Tuning with Sentence Transformers\n",
    "\n",
    "In this lab, we will explore transfer learning and fine-tuning techniques using pre-trained Sentence Transformers.\n",
    "Our goal is to classify Wikipedia article summaries into specific categories using a fully connected neural network\n",
    "layer added to the pre-trained model.\n",
    "\n",
    "## Objectives\n",
    "1. **Understand Transfer Learning**: Learn how to leverage pre-trained models for new tasks.\n",
    "2. **Fine-Tuning Models**: Add and train new layers on top of a pre-trained model.\n",
    "3. **Optimize Training**: Experiment with different training parameters to achieve the best performance.\n",
    "4. **Evaluate Model**: Test the model's performance on both fitting and non-fitting topics.\n",
    "\n",
    "## Sections\n",
    "1. **Setup and Imports**: Import necessary libraries and modules.\n",
    "2. **Loading Pre-trained Model**: Load a pre-trained Sentence Transformer model.\n",
    "3. **Extending the Model**: Add a fully connected layer to the pre-trained model.\n",
    "4. **Defining Loss and Optimizer**: Set up the loss function, optimizer, and learning rate scheduler.\n",
    "5. **Training the Model**: Train the extended model using the provided data.\n",
    "6. **Evaluating the Model**: Check model predictions on new topics.\n",
    "7. **Challenge**: Optimize the training process and analyze the results.\n",
    "\n",
    "## Instructions\n",
    "Follow the steps in each section, running the provided code and completing the challenges. Make sure to document your\n",
    "experiments, results, and analyses as you proceed.\n",
    "\n",
    "## Setup and Imports\n",
    "Let's start by importing the necessary libraries and modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718632c4",
   "metadata": {},
   "source": [
    "!pip install sentence-transformers wikipedia -q --upgrade\n",
    "!wget -q https://raw.githubusercontent.com/marr75/wecodekc-scientific-computing/main/2024/lab-02/summaries.pkl\n",
    "!wget -q https://raw.githubusercontent.com/marr75/wecodekc-scientific-computing/main/2024/lab-02/word_list.py"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4883fbd",
   "metadata": {},
   "source": [
    "# fmt: off\n",
    "# Allows for forward references in type hints, improving code readability and maintainability.\n",
    "from __future__ import annotations\n",
    "# Provides support for creating enumerations, which are a set of symbolic names bound to unique, constant values.\n",
    "import enum\n",
    "# Contains functions that create iterators for efficient looping, such as chain, cycle, and permutations.\n",
    "import itertools\n",
    "import pickle\n",
    "\n",
    "# A deep learning framework that provides tensors and dynamic neural networks in Python with strong GPU acceleration.\n",
    "import torch\n",
    "# A library for easy-to-use pre-trained sentence embedding models.\n",
    "import sentence_transformers\n",
    "# A Python wrapper for the Wikipedia API, useful for extracting data from Wikipedia articles.\n",
    "import wikipedia\n",
    "\n",
    "# Custom word list for the lab\n",
    "import word_list\n",
    "\n",
    "# fmt: on"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491802e1",
   "metadata": {},
   "source": [
    "# We've used this model in the previous labs\n",
    "base_model_name = \"avsolatorio/GIST-small-Embedding-v0\"\n",
    "# Load a pre-trained Sentence Transformer model\n",
    "base_model = sentence_transformers.SentenceTransformer(base_model_name)\n",
    "# Get the device (typically CPU or GPU) on which the model is loaded, we'll use this device for training\n",
    "device = base_model.device"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c749a29",
   "metadata": {},
   "source": [
    "# Define a new model by adding a fully connected layer\n",
    "class ExtendedModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A model that extends a pre-trained Sentence Transformer model with a fully connected layer.\n",
    "    \"\"\"\n",
    "\n",
    "    # Init method, called when an instance of the class is created\n",
    "    def __init__(self, base_model: sentence_transformers.SentenceTransformer, output_dim: int) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the model with a pre-trained Sentence Transformer model and a fully connected layer.\n",
    "        \"\"\"\n",
    "        super(ExtendedModel, self).__init__()\n",
    "        # Use the pre-trained Sentence Transformer model\n",
    "        self.base_model = base_model\n",
    "        # Add a fully connected layer\n",
    "        self.fc = torch.nn.Linear(base_model.get_sentence_embedding_dimension(), output_dim)\n",
    "\n",
    "    def forward(self, in_: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the model. Passes the input through the base model and then through the fully connected layer.\n",
    "        \"\"\"\n",
    "        # Ensure parameters are not updated\n",
    "        with torch.no_grad():\n",
    "            # Get embeddings\n",
    "            embeddings = self.base_model.encode(in_, convert_to_tensor=True)\n",
    "        # Pass embeddings through the fully connected layer\n",
    "        output = self.fc(embeddings)\n",
    "        return output\n",
    "\n",
    "\n",
    "# Set the output dimension; this should match the number of classes in the classification task\n",
    "output_dim = 8\n",
    "# Instantiate the extended model and move it to the device (CPU or GPU)\n",
    "model = ExtendedModel(base_model, output_dim).to(device)\n",
    "# Freeze the base model parameters to avoid updating them during training\n",
    "model.base_model.requires_grad_(False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8610624",
   "metadata": {},
   "source": [
    "# Let's take a look at the models\n",
    "model"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5cc97c44",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "The output of the `model` shows the structure of our `ExtendedModel` class. It includes:\n",
    "\n",
    "- `base_model`: The pre-trained Sentence Transformer, which is composed of:\n",
    "  - A `Transformer` layer for encoding sentences.\n",
    "  - A `Pooling` layer for aggregating the output of the transformer model.\n",
    "  - A `Normalize` layer for normalizing the embeddings.\n",
    "- `fc`: A fully connected (`Linear`) layer that maps the sentence embeddings (with an input dimension of 384) to our\n",
    "specified output dimension (8 in this case).\n",
    "\n",
    "The parameters of the `base_model` are frozen and won't be updated during training, allowing us to focus on training\n",
    "the `fc` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e82d3e1",
   "metadata": {},
   "source": [
    "def train_model(\n",
    "    model: torch.nn.Module,\n",
    "    loss_function: torch.nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    train_data: list[str],\n",
    "    train_labels: torch.Tensor,\n",
    "    epochs: int = 10,\n",
    "    scheduler: torch.optim.lr_scheduler._LRScheduler | None = None,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Train the model using the provided data and labels.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to be trained.\n",
    "        loss_function (torch.nn.Module): The loss function.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer for updating the model parameters.\n",
    "        train_data (torch.Tensor): The training data.\n",
    "        train_labels (torch.Tensor): The training labels.\n",
    "        epochs (int, optional): The number of training epochs. Defaults to 10.\n",
    "        scheduler (torch.optim.lr_scheduler._LRScheduler | None, optional): The learning rate scheduler.\n",
    "    \"\"\"\n",
    "    model.train()  # Set the model to training mode\n",
    "    for epoch in range(epochs):\n",
    "        # Reset the gradients to zero, don't want to accumulate them\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass: compute predictions\n",
    "        predictions = model(train_data)\n",
    "        # Compute the loss\n",
    "        loss = loss_function(predictions, train_labels)\n",
    "        # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")  # Print training progress"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22404261",
   "metadata": {},
   "source": [
    "class ArticleLabels(enum.Enum):\n",
    "    \"\"\"Enum for article labels.\"\"\"\n",
    "\n",
    "    ATHLETES = 0\n",
    "    DISHES = 1\n",
    "    ACTORS = 2\n",
    "    MUSIC_ARTISTS = 3\n",
    "    HISTORICAL_FIGURES = 4\n",
    "    JOBS = 5\n",
    "    COUNTRIES = 6\n",
    "    COLORS = 7\n",
    "\n",
    "\n",
    "# Load training data from a pickle file\n",
    "training_data = [summary for _, summary in pickle.load(open(\"2024/lab-02/summaries.pkl\", \"rb\"))]\n",
    "\n",
    "# Create training labels tensor\n",
    "training_labels = torch.tensor(\n",
    "    list(\n",
    "        # Chain the labels for each category\n",
    "        itertools.chain(\n",
    "            (ArticleLabels.ATHLETES.value for _ in word_list.athletes),\n",
    "            (ArticleLabels.DISHES.value for _ in word_list.dishes),\n",
    "            (ArticleLabels.ACTORS.value for _ in word_list.actors),\n",
    "            (ArticleLabels.MUSIC_ARTISTS.value for _ in word_list.music_artists),\n",
    "            (ArticleLabels.HISTORICAL_FIGURES.value for _ in word_list.historical_figures),\n",
    "            (ArticleLabels.JOBS.value for _ in word_list.jobs),\n",
    "            (ArticleLabels.COUNTRIES.value for _ in word_list.countries),\n",
    "            (ArticleLabels.COLORS.value for _ in word_list.colors),\n",
    "        )\n",
    "    )\n",
    ").to(device)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "887c20b0",
   "metadata": {},
   "source": [
    "## Challenge: Optimize the Training Process\n",
    "\n",
    "In this challenge, your goal is to achieve the best performing model possible within a fixed number of epochs (30).\n",
    "You can experiment with different optimizers, learning rates, and other parameters to see how they affect the training\n",
    "process.\n",
    "\n",
    "### Levers You Can Use:\n",
    "- **Optimizer**: Try different optimizers such as Adam, SGD, RMSprop, etc.\n",
    "- **Learning Rate**: Adjust the learning rate to find the optimal value.\n",
    "- **Momentum**: If using SGD, experiment with different momentum values.\n",
    "- **Scheduler**: Use learning rate schedulers to adjust the learning rate during training.\n",
    "\n",
    "### Example Optimizer Configurations:\n",
    "```python\n",
    "# Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.1)\n",
    "\n",
    "# SGD optimizer with momentum\n",
    "optimizer = torch.optim.SGD(model.fc.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# RMSprop optimizer\n",
    "optimizer = torch.optim.RMSprop(model.fc.parameters(), lr=0.001, alpha=0.99)\n",
    "```\n",
    "\n",
    "### Example Scheduler Configurations:\n",
    "```python\n",
    "# StepLR scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# ExponentialLR scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "# CosineAnnealingLR scheduler\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "```\n",
    "\n",
    "### Your Task:\n",
    "- Experiment with different configurations to find the best performing model.\n",
    "- Document what you tried, your results, and provide a brief analysis.\n",
    "\n",
    "### Documentation:\n",
    "1. **What You Tried**:\n",
    "   - Optimizer: SGD with learning rate 0.01 and momentum 0.9.\n",
    "   - Learning rate scheduler: StepLR with step size 10 and gamma 0.1.\n",
    "   - Epochs: 30.\n",
    "\n",
    "2. **Results**:\n",
    "   - Training Loss after 30 epochs: 0.2.\n",
    "\n",
    "3. **Analysis**:\n",
    "   - The model performed better with SGD and momentum compared to Adam. The lower learning rate helped stabilize the\n",
    "     training process, resulting in a lower final training loss.\n",
    "\n",
    "Good luck, and happy training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50db50d4",
   "metadata": {},
   "source": [
    "# Define loss function and optimizer\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizers: Uncomment and modify the optimizer you want to try\n",
    "# SGD = Stochastic Gradient Descent, an optimization algorithm that updates the parameters based on the computed\n",
    "#   gradients, the learning rate, and some randomness\n",
    "# SGD with momentum, an optimization algorithm that adds a fraction of the update vector of the past time step to the\n",
    "#   current update vector\n",
    "optimizer = torch.optim.SGD(model.fc.parameters(), lr=0.01, momentum=0.9)\n",
    "# Adam, an optimization algorithm that is an extension of stochastic gradient descent\n",
    "# optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.1)\n",
    "# RMSprop, an optimization algorithm that divides the gradient by a running average of its recent magnitude\n",
    "# optimizer = torch.optim.RMSprop(model.fc.parameters(), lr=0.001, alpha=0.99)\n",
    "\n",
    "# Learning rate scheduler: Uncomment the scheduler you want to use\n",
    "# StepLR, decays the learning rate by gamma every step_size epochs\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "# MultiStepLR, decays the learning rate by gamma at each milestone epochs\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[2, 4], gamma=0.1)\n",
    "# ExponentialLR, decays the learning rate by gamma every epoch\n",
    "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "# CosineAnnealingLR, anneals the learning rate following the cosine function\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5, eta_min=0)\n",
    "# ReduceLROnPlateau, Decrease the learning rate when the loss reaches a plateau\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.1, patience=10, verbose=True)\n",
    "scheduler = None\n",
    "\n",
    "number_of_epochs = 30\n",
    "train_model(\n",
    "    model, loss_function, optimizer, training_data, training_labels, epochs=number_of_epochs, scheduler=scheduler\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e33526",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "def predict(model: torch.nn.Module, input_data: str) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generate predictions for the given input data.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The trained model.\n",
    "        input_data (torch.Tensor): The input data for which to generate predictions.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The predicted probabilities for each class.\n",
    "    \"\"\"\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    # Disable gradient computation\n",
    "    with torch.no_grad():\n",
    "        # Forward pass: compute logits\n",
    "        logits = model(input_data)\n",
    "        # Apply softmax to get probabilities\n",
    "        probabilities = torch.softmax(logits, dim=-1)\n",
    "    # Move the result to CPU and return\n",
    "    return probabilities.to(\"cpu\")\n",
    "\n",
    "\n",
    "def format_predictions(probabilities: torch.Tensor, labels_enum: enum.EnumMeta) -> dict[str, float]:\n",
    "    \"\"\"\n",
    "    Format the predicted probabilities into a readable dictionary.\n",
    "\n",
    "    Args:\n",
    "        probabilities (torch.Tensor): The predicted probabilities.\n",
    "        labels_enum (enum.EnumMeta): The enumeration of label names.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, float]: A dictionary mapping label names to their predicted probabilities.\n",
    "    \"\"\"\n",
    "    probs = probabilities.squeeze().tolist()  # Convert probabilities to list\n",
    "    # Map each probability to its corresponding label name\n",
    "    label_probs = {labels_enum(i).name: prob for i, prob in enumerate(probs)}\n",
    "    return label_probs  # Return the formatted predictions\n",
    "\n",
    "\n",
    "def safe_summary(title: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Get the summary of a Wikipedia page safely\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return wikipedia.summary(title, auto_suggest=False)\n",
    "    except wikipedia.exceptions.PageError:\n",
    "        print(f\"Page '{title}' does not exist.\")\n",
    "        return None\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        print(f\"Page '{title}' is ambiguous. Did you mean one of these?\\ne: {e.options}\")\n",
    "        return None"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "91961bce",
   "metadata": {},
   "source": [
    "## Checking Predictions on Novel Topics\n",
    "\n",
    "In this section, we will test the model's predictions on new topics to see how well it categorizes them. We'll use\n",
    "topics that should fit into the existing categories from the `word_list` as well as topics that shouldn't.\n",
    "\n",
    "### Examples:\n",
    "- **Topics that should fit**:\n",
    "  - **Patrick Mahomes**: As an athlete, the model should categorize him under `ATHLETES`.\n",
    "  - **Sushi**: As a type of food, the model should categorize it under `DISHES`.\n",
    "\n",
    "- **Topics that shouldn't fit**:\n",
    "  - **Quantum Computing**: This topic is unrelated to any of our categories and should ideally be classified with low\n",
    "    confidence across all categories.\n",
    "  - **Climate Change**: Similar to quantum computing, this topic doesn't fit into any of our predefined categories.\n",
    "\n",
    "### Instructions:\n",
    "1. Use the function `predict_and_format` to check the predictions for various topics.\n",
    "2. Analyze the model's performance on both fitting and non-fitting topics.\n",
    "3. Document your findings and provide a brief analysis.\n",
    "\n",
    "### Example Code:\n",
    "```python\n",
    "predictions = predict(model, safe_summary(\"Patrick Mahomes\"))\n",
    "format_predictions(predictions, ArticleLabels)\n",
    "\n",
    "predictions = predict(model, safe_summary(\"Sushi\"))\n",
    "print(predictions)\n",
    "\n",
    "predictions = predict(model, safe_summary(\"Quantum Computing\"))\n",
    "format_predictions(predictions, ArticleLabels)\n",
    "\n",
    "predictions = predict(model, safe_summary(\"Climate Change\"))\n",
    "format_predictions(predictions, ArticleLabels)\n",
    "```\n",
    "\n",
    "By testing the model on these novel topics, we can gain insights into its generalization capabilities and identify\n",
    "any potential biases or limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a81482d",
   "metadata": {},
   "source": [
    "predictions = predict(model, safe_summary(\"Patrick Mahomes\"))\n",
    "format_predictions(predictions, ArticleLabels)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "28cb6b68",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, we explored various concepts and techniques related to transfer learning and fine-tuning models. Here’s\n",
    "a summary of what we learned:\n",
    "\n",
    "### Transfer Learning\n",
    "- **Pre-trained Models**: We leveraged a pre-trained Sentence Transformer model to encode Wikipedia article summaries\n",
    "  into embeddings.\n",
    "- **Extending Models**: By adding a fully connected layer to the pre-trained model, we adapted it for our specific\n",
    "  classification task.\n",
    "\n",
    "### Fine-Tuning Models\n",
    "- **Freezing Layers**: We froze the parameters of the pre-trained model to focus the training on the newly added\n",
    "  layer.\n",
    "- **Training Process**: We defined a training loop that included loss computation, backpropagation, and optimizer\n",
    "  steps.\n",
    "- **Optimizing Training**: We experimented with different optimizers, learning rates, and schedulers to achieve the\n",
    "  best performance.\n",
    "\n",
    "### Classes and Functions\n",
    "- **Custom Classes**: We created custom classes such as `ExtendedModel` for extending the pre-trained model and\n",
    "  `ArticleLabels` for categorizing the articles.\n",
    "- **Helper Functions**: Functions like `train_model`, `predict`, and `format_predictions` helped structure our code\n",
    "  and made it reusable.\n",
    "\n",
    "### Using Libraries\n",
    "- **PyTorch**: We used PyTorch for building and training our neural network model.\n",
    "- **Sentence Transformers**: This library provided easy-to-use pre-trained models for generating sentence embeddings.\n",
    "- **Wikipedia**: By using the Wikipedia API, we efficiently sourced article summaries for training and evaluation.\n",
    "- **Custom Utilities**: Modules like `lab_utilities` and `word_list` helped streamline our workflow by providing\n",
    "  necessary data and helper functions.\n",
    "\n",
    "### Practical Application\n",
    "- **Novel Topics Evaluation**: We tested the model’s generalization capabilities by predicting categories for new\n",
    "  topics, both fitting and non-fitting.\n",
    "- **Documentation and Analysis**: We emphasized the importance of documenting experiments, analyzing results, and\n",
    "  understanding the impact of different training parameters.\n",
    "\n",
    "### Challenge and Experimentation\n",
    "- **Optimizing the Model**: We challenged ourselves to find the best performing model within a fixed number of epochs\n",
    "  by experimenting with various training configurations.\n",
    "\n",
    "## Key Takeaways\n",
    "- **Transfer Learning**: A powerful technique to adapt pre-trained models for new tasks.\n",
    "- **Model Fine-Tuning**: Essential for improving model performance on specific tasks.\n",
    "- **Structured Coding**: Writing reusable classes and functions enhances code readability and maintainability.\n",
    "- **Libraries and APIs**: Leveraging existing libraries and APIs can significantly speed up the development process.\n",
    "\n",
    "By completing this lab, you have gained hands-on experience with transfer learning, fine-tuning models, and\n",
    "optimizing training processes. These skills are valuable for developing efficient and effective machine learning\n",
    "models for a wide range of applications."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
