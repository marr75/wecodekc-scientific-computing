{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bd07151",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Section 2: timm/PyTorch Image Model Library\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Welcome to the second section of our lab! Today, we’ll explore timm, a library for loading pre-trained image models\n",
    "using PyTorch. Pre-trained models have already been trained on large datasets and can be used to extract useful\n",
    "features from new images. We’ll learn how to use these models to encode images into embeddings and explore some\n",
    "exciting applications.\n",
    "\n",
    "Activities\n",
    "1. Introduction to timm\n",
    "  • We’ll start by introducing the timm library and explaining its purpose in the world of machine learning.\n",
    "  • Placeholder for URL to timm documentation.\n",
    "2. Load a Pre-trained Model\n",
    "  • Using timm, we’ll load a pre-trained model and use it to encode a set of images into embeddings.\n",
    "  • Embeddings are compact, dense representations of images that capture their essential features.\n",
    "3. Image Search Use Case\n",
    "  • We’ll implement an image search use case to show how embeddings can be used to find similar images.\n",
    "  • This will demonstrate the power and practicality of using pre-trained models.\n",
    "4. 3D Visualization with UMAP\n",
    "  • We’ll use UMAP to create a 3D plot of the image embeddings.\n",
    "  • UMAP (Uniform Manifold Approximation and Projection) is a technique for dimensionality reduction that helps us\n",
    "  visualize high-dimensional data in a lower-dimensional space.\n",
    "  • We’ll investigate and discuss the clusters of images in the plot.\n",
    "\n",
    "Key Points\n",
    "\n",
    " • timm Library: A tool for accessing pre-trained image models.\n",
    " • Image Embeddings: Compact representations of images used for various applications.\n",
    " • Image Search: Finding similar images using embeddings.\n",
    " • UMAP: A technique for visualizing high-dimensional data.\n",
    "\n",
    "Let’s begin by learning about the timm library!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ebabfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets timm umap-learn plotly pandas --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eef5dd8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import timm.data\n",
    "import torch\n",
    "import umap\n",
    "from IPython.display import display\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263f8fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for image embeddings\n",
    "\n",
    "\n",
    "def get_image_embeddings(images: list[Image.Image], model: torch.nn.Module, transforms: Any) -> list[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Get the embeddings of a batch of images using a pre-trained model.\n",
    "\n",
    "    Args:\n",
    "        images (List[Image.Image]): The input images.\n",
    "        model (torch.nn.Module): The pre-trained model.\n",
    "        transforms: The image transformations.\n",
    "\n",
    "    Returns:\n",
    "        List[np.ndarray]: The embeddings of the images.\n",
    "    \"\"\"\n",
    "    img_tensors = torch.stack([transforms(image) for image in images])\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(img_tensors).detach().cpu().numpy()\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def reduce_embeddings(embeddings: np.ndarray, reducer: umap.UMAP, fit: bool = True) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Reduce the dimensionality of embeddings using UMAP.\n",
    "\n",
    "    Args:\n",
    "        embeddings (np.ndarray): The input embeddings.\n",
    "        reducer (umap.UMAP): The UMAP reducer.\n",
    "        fit (bool): Whether to fit the reducer on the embeddings.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The reduced embeddings.\n",
    "    \"\"\"\n",
    "    if fit:\n",
    "        reduced_embeddings = reducer.fit_transform(embeddings)\n",
    "    else:\n",
    "        reduced_embeddings = reducer.transform(embeddings)\n",
    "    return reduced_embeddings\n",
    "\n",
    "\n",
    "def make_dataframe_from_images(dataset: datasets.Dataset) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a DataFrame from a dataset with embeddings and reduced embeddings.\n",
    "\n",
    "    Args:\n",
    "        dataset: The dataset containing images, embeddings, and reduced embeddings.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing titles, images, and reduced embeddings.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"label\": example[\"label\"],\n",
    "                \"label_name\": example[\"label_name\"],\n",
    "                \"image\": example[\"image\"],\n",
    "                \"x\": example[\"reduced_embedding\"][0],\n",
    "                \"y\": example[\"reduced_embedding\"][1],\n",
    "                \"z\": example[\"reduced_embedding\"][2],\n",
    "            }\n",
    "            for example in dataset\n",
    "        ]\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def plot_image_embeddings(embeddings_df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Plot the image embeddings DataFrame in 3D.\n",
    "    \"\"\"\n",
    "    fig = px.scatter_3d(\n",
    "        embeddings_df, x=\"x\", y=\"y\", z=\"z\", hover_data=[\"label_name\", \"image\"], size_max=60, template=\"plotly_white\"\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        title=\"Image Embeddings Visualization\",\n",
    "        xaxis_title=\"Component 1\",\n",
    "        yaxis_title=\"Component 2\",\n",
    "        legend_title=\"Summaries\",\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def create_timm_model(model_name: str = \"convnextv2_base.fcmae\") -> (torch.nn.Module, any):\n",
    "    \"\"\"\n",
    "    Create a TIMM model for extracting embeddings and get the appropriate transforms.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The name of the pre-trained model to load.\n",
    "\n",
    "    Returns:\n",
    "        model (torch.nn.Module): The pre-trained model.\n",
    "        transforms: The transformations to apply to the images.\n",
    "    \"\"\"\n",
    "    model = timm.create_model(\n",
    "        model_name,\n",
    "        pretrained=True,\n",
    "        num_classes=0,  # remove classifier nn.Linear\n",
    "    )\n",
    "    model = model.eval()\n",
    "\n",
    "    # Get model specific transforms (normalization, resize)\n",
    "    data_config = timm.data.resolve_model_data_config(model)\n",
    "    transforms = timm.data.create_transform(**data_config, is_training=False, normalize=True)\n",
    "\n",
    "    return model, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6014dd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our model and get the transforms\n",
    "model, transforms = create_timm_model()\n",
    "display(model, transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d535f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Food101 dataset\n",
    "food101 = datasets.load_dataset(\"food101\")\n",
    "# Convert food101 class labels from integer to descriptive string\n",
    "label_names = food101[\"train\"].features[\"label\"].names\n",
    "food101 = food101.map(lambda x: {\"label_name\": label_names[x[\"label\"]]})\n",
    "display(food101, food101[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662f2bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample of 50 examples from the Food101 dataset\n",
    "food101_sample = food101[\"train\"].shuffle(seed=42).select(range(30))\n",
    "embeddings = get_image_embeddings(food101_sample[\"image\"], model, transforms)\n",
    "reducer = umap.UMAP(n_components=3)\n",
    "reduced_embeddings = reduce_embeddings(embeddings, reducer, fit=True)\n",
    "embedded_and_reduced_images = food101_sample.map(\n",
    "    lambda x, i: {\"embedding\": embeddings[i], \"reduced_embedding\": reduced_embeddings[i]},\n",
    "    with_indices=True,\n",
    ")\n",
    "display(embedded_and_reduced_images[0])\n",
    "df = make_dataframe_from_images(embedded_and_reduced_images)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd2916c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the image embeddings in 3D\n",
    "plot_image_embeddings(df)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
