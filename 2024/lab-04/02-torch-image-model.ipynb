{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a7cc25d",
   "metadata": {},
   "source": [
    "## Section 2: timm/PyTorch Image Model Library\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Welcome to the second section of our lab! Today, we’ll explore timm, a library for loading pre-trained image models\n",
    "using PyTorch. Pre-trained models have already been trained on large datasets and can be used to extract useful\n",
    "features from new images. We’ll learn how to use these models to encode images into embeddings and explore some\n",
    "exciting applications.\n",
    "\n",
    "Activities\n",
    "1. Introduction to timm\n",
    "  • We’ll start by introducing the timm library and explaining its purpose in the world of machine learning.\n",
    "  • Placeholder for URL to timm documentation.\n",
    "2. Load a Pre-trained Model\n",
    "  • Using timm, we’ll load a pre-trained model and use it to encode a set of images into embeddings.\n",
    "  • Embeddings are compact, dense representations of images that capture their essential features.\n",
    "3. Image Search Use Case\n",
    "  • We’ll implement an image search use case to show how embeddings can be used to find similar images.\n",
    "  • This will demonstrate the power and practicality of using pre-trained models.\n",
    "4. 3D Visualization with UMAP\n",
    "  • We’ll use UMAP to create a 3D plot of the image embeddings.\n",
    "  • UMAP (Uniform Manifold Approximation and Projection) is a technique for dimensionality reduction that helps us\n",
    "  visualize high-dimensional data in a lower-dimensional space.\n",
    "  • We’ll investigate and discuss the clusters of images in the plot.\n",
    "\n",
    "Key Points\n",
    "\n",
    " • timm Library: A tool for accessing pre-trained image models.\n",
    " • Image Embeddings: Compact representations of images used for various applications.\n",
    " • Image Search: Finding similar images using embeddings.\n",
    " • UMAP: A technique for visualizing high-dimensional data.\n",
    "\n",
    "Let’s begin by learning about the timm library!\n",
    "import base64\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f394f3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets timm umap-learn plotly pandas --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744f4b89",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import timm.data\n",
    "import torch\n",
    "import umap\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f979302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for image embeddings\n",
    "\n",
    "\n",
    "def get_image_embeddings(images: list[Image.Image], model: torch.nn.Module, transforms: Any) -> list[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Get the embeddings of a batch of images using a pre-trained model.\n",
    "\n",
    "    Args:\n",
    "        images (List[Image.Image]): The input images.\n",
    "        model (torch.nn.Module): The pre-trained model.\n",
    "        transforms: The image transformations.\n",
    "\n",
    "    Returns:\n",
    "        List[np.ndarray]: The embeddings of the images.\n",
    "    \"\"\"\n",
    "    img_tensors = torch.stack([transforms(image) for image in images])\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(img_tensors).detach().cpu().numpy()\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def reduce_embeddings(embeddings: np.ndarray, reducer: umap.UMAP, fit: bool = True) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Reduce the dimensionality of embeddings using UMAP.\n",
    "\n",
    "    Args:\n",
    "        embeddings (np.ndarray): The input embeddings.\n",
    "        reducer (umap.UMAP): The UMAP reducer.\n",
    "        fit (bool): Whether to fit the reducer on the embeddings.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The reduced embeddings.\n",
    "    \"\"\"\n",
    "    if fit:\n",
    "        reduced_embeddings = reducer.fit_transform(embeddings)\n",
    "    else:\n",
    "        reduced_embeddings = reducer.transform(embeddings)\n",
    "    return reduced_embeddings\n",
    "\n",
    "\n",
    "def pil_to_base64(image: Image.Image) -> str:\n",
    "    \"\"\"\n",
    "    Convert a PIL Image to a base64 encoded string.\n",
    "\n",
    "    Args:\n",
    "        image (Image.Image): The input image.\n",
    "\n",
    "    Returns:\n",
    "        str: The base64 encoded string of the image.\n",
    "    \"\"\"\n",
    "    buffer = io.BytesIO()\n",
    "    image.save(buffer, format=\"PNG\")\n",
    "    img_str = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
    "    return f\"data:image/png;base64,{img_str}\"\n",
    "\n",
    "\n",
    "def make_dataframe_from_images(dataset: datasets.Dataset) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a DataFrame from a dataset with embeddings and reduced embeddings.\n",
    "\n",
    "    Args:\n",
    "        dataset: The dataset containing images, embeddings, and reduced embeddings.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing titles, images, and reduced embeddings.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"label\": example[\"label\"],\n",
    "                \"label_name\": example[\"label_name\"],\n",
    "                \"image\": example[\"b64_image\"],\n",
    "                \"x\": example[\"reduced_embedding\"][0],\n",
    "                \"y\": example[\"reduced_embedding\"][1],\n",
    "                \"z\": example[\"reduced_embedding\"][2],\n",
    "            }\n",
    "            for example in dataset\n",
    "        ]\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def plot_image_embeddings(embeddings_df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Plot the image embeddings DataFrame in 3D.\n",
    "    \"\"\"\n",
    "    fig = px.scatter_3d(\n",
    "        embeddings_df, x=\"x\", y=\"y\", z=\"z\", hover_data=[\"label_name\"], size_max=60, template=\"plotly_white\"\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        title=\"Image Embeddings Visualization\",\n",
    "        xaxis_title=\"Component 1\",\n",
    "        yaxis_title=\"Component 2\",\n",
    "        legend_title=\"Summaries\",\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def load_timm_model(model_name: str = \"convnextv2_base.fcmae\") -> (torch.nn.Module, any):\n",
    "    \"\"\"\n",
    "    Create a TIMM model for extracting embeddings and get the appropriate transforms.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The name of the pre-trained model to load.\n",
    "\n",
    "    Returns:\n",
    "        model (torch.nn.Module): The pre-trained model.\n",
    "        transforms: The transformations to apply to the images.\n",
    "    \"\"\"\n",
    "    model = timm.create_model(\n",
    "        model_name,\n",
    "        pretrained=True,\n",
    "        num_classes=0,  # remove classifier nn.Linear\n",
    "    )\n",
    "    model = model.eval()\n",
    "\n",
    "    # Get model specific transforms (normalization, resize)\n",
    "    data_config = timm.data.resolve_model_data_config(model)\n",
    "    transforms = timm.data.create_transform(**data_config, is_training=False, normalize=True)\n",
    "\n",
    "    return model, transforms\n",
    "\n",
    "\n",
    "def calculate_cosine_similarity(embedding: np.ndarray, embeddings: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between a single embedding and a set of embeddings.\n",
    "\n",
    "    Args:\n",
    "        embedding (np.ndarray): The embedding of the query image.\n",
    "        embeddings (np.ndarray): The embeddings of all images in the dataset.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Cosine similarity scores.\n",
    "    \"\"\"\n",
    "    embedding = embedding.reshape(1, -1)  # Reshape to 2D array for cosine similarity calculation\n",
    "    similarities = cosine_similarity(embedding, embeddings)\n",
    "    return similarities.flatten()\n",
    "\n",
    "\n",
    "def find_top_n_similar_images(query_embedding: np.ndarray, embeddings: np.ndarray, n: int = 5) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Find the top N most similar images based on cosine similarity.\n",
    "\n",
    "    Args:\n",
    "        query_embedding (np.ndarray): The embedding of the query image.\n",
    "        embeddings (np.ndarray): The embeddings of all images in the dataset.\n",
    "        n (int): The number of top similar images to return.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Indices of the top N most similar images.\n",
    "    \"\"\"\n",
    "    similarities = calculate_cosine_similarity(query_embedding, embeddings)\n",
    "    top_n_indices = np.argsort(similarities)[-n:][::-1]  # Get indices of top N similar images, sorted by similarity\n",
    "    return top_n_indices\n",
    "\n",
    "\n",
    "def search_similar_images(\n",
    "    query_image: Image.Image, model: torch.nn.Module, transforms: Any, dataset: datasets.Dataset, top_n: int = 5\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Search for the top N most similar images in the dataset given a query image.\n",
    "\n",
    "    Args:\n",
    "        query_image (Image.Image): The query image.\n",
    "        model (torch.nn.Module): The pre-trained model.\n",
    "        transforms: The image transformations.\n",
    "        dataset: The dataset containing images and embeddings.\n",
    "        top_n (int): The number of top similar images to return.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the top N similar images and their details.\n",
    "    \"\"\"\n",
    "    query_embedding = get_image_embeddings([query_image], model, transforms)[0]\n",
    "    embeddings = np.array([example[\"embedding\"] for example in dataset])\n",
    "    top_n_indices = find_top_n_similar_images(query_embedding, embeddings, n=top_n)\n",
    "\n",
    "    similar_images = [dataset[int(i)] for i in top_n_indices]\n",
    "    df = make_dataframe_from_images(similar_images)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b00477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our model and get the transforms\n",
    "model, transforms = load_timm_model()\n",
    "display(model, transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8e0d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Food101 dataset\n",
    "food101 = datasets.load_dataset(\"food101\")\n",
    "# Convert food101 class labels from integer to descriptive string\n",
    "label_names = food101[\"train\"].features[\"label\"].names\n",
    "food101 = food101.map(lambda x: {\"label_name\": label_names[x[\"label\"]]})\n",
    "display(food101, food101[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5108c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample of 50 examples from the Food101 dataset\n",
    "how_many = 50\n",
    "food101_sample = food101[\"train\"].shuffle(seed=42).select(range(how_many))\n",
    "embeddings = get_image_embeddings(food101_sample[\"image\"], model, transforms)\n",
    "reducer = umap.UMAP(n_components=3)\n",
    "reduced_embeddings = reduce_embeddings(embeddings, reducer, fit=True)\n",
    "embedded_and_reduced_images = food101_sample.map(\n",
    "    lambda x, i: {\n",
    "        \"embedding\": embeddings[i],\n",
    "        \"reduced_embedding\": reduced_embeddings[i],\n",
    "        \"b64_image\": pil_to_base64(x[\"image\"]),\n",
    "    },\n",
    "    with_indices=True,\n",
    ")\n",
    "display(embedded_and_reduced_images[0])\n",
    "df = make_dataframe_from_images(embedded_and_reduced_images)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046be91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the image embeddings in 3D\n",
    "plot_image_embeddings(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1e8863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the image you want to search for similar images\n",
    "query_index = 0\n",
    "query_image = food101_sample[query_index][\"image\"]\n",
    "display(query_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c0ecf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for similar images to the query image\n",
    "similar_images_df = search_similar_images(query_image, model, transforms, embedded_and_reduced_images)\n",
    "display(similar_images_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d8fda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Find an image of your favorite food on the internet and use it as a query image to search for similar\n",
    "# images in the Food101 dataset. You can download the image and load it using the PIL library. Replace the query_image\n",
    "# variable with your image and run the cell to see the results.\n",
    "# Define the url of the image you want to use as a query here\n",
    "\n",
    "# Load the image using PIL\n",
    "\n",
    "# Search for similar images to the query image\n",
    "\n",
    "# Display the results\n",
    "\n",
    "# Question: How well did the model perform in finding similar images to your query image?"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
